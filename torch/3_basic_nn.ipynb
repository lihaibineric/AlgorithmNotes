{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module): # 继承nn.Module，必须重写构造函数（__init__)和前向传播函数（forward）\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__() # 等价于nn.Module.__init__(self)，常用super方式\n",
    "        # nn.Parameter内的参数是网络可学习的参数\n",
    "        self.w = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w) # 矩阵乘法，等价于x.@(self.w)\n",
    "        return x + self.b.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6353, -0.5164, -0.9270],\n",
       "        [-4.0045, -1.1728,  0.2221]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Linear(4,3)\n",
    "input = torch.randn(2,4)\n",
    "output = layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-0.3632, -0.5640,  0.7314],\n",
      "        [-1.9605, -0.4759, -0.5635],\n",
      "        [-2.2911,  0.3091, -0.2229],\n",
      "        [ 0.5920,  0.1634, -0.3330]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([ 1.6898, -0.7530, -0.2323], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in layer.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置运行设备device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: mps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" \n",
    "    if torch.backends.mps.is_available() # MacOS上的训练模块\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"The device is: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Module放在GPU上运行也十分简单，只需以下两步："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = model.cuda() #将模型的所有参数转存到GPU；\n",
    "input.cuda() #将输入数据放置到GPU上。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于如何在多个GPU上并行计算，PyTorch也提供了两个函数，可实现简单高效的并行GPU计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构的写法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential类型写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (cov1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = nn.Sequential()\n",
    "net1.add_module('cov1',nn.Conv2d(3, 3, 3))\n",
    "net1.add_module('bn', nn.BatchNorm2d(3))\n",
    "net1.add_module('relu',nn.ReLU())\n",
    "print(net1)\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "    nn.Conv2d(3,3,3),\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "print(net2)\n",
    "\n",
    "from collections import OrderedDict\n",
    "net3= nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(3, 3, 3)),\n",
    "          ('bn1', nn.BatchNorm2d(3)),\n",
    "          ('relu1', nn.ReLU())\n",
    "        ]))\n",
    "print(net3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以根据对应的名称或者位置取出网络的对应的层数具体信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(net1.cov1, net2[0], net3.conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化NN的Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module): # 继承nn.Module，必须重写构造函数（__init__)和前向传播函数（forward）\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__() # 等价于nn.Module.__init__(self)，常用super方式\n",
    "        # nn.Parameter内的参数是网络可学习的参数\n",
    "        self.w = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        # self.w = torch.randn(in_features, out_features, requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w) # 矩阵乘法，等价于x.@(self.w)\n",
    "        return x + self.b.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化网络并输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "shape = (1, 28, 28) # 注意这里的1 表示bs = 1\n",
    "x = torch.rand(shape, device=device)\n",
    "logits = model(x) # (1,10)\n",
    "pred_prob = nn.Softmax(dim = 1)(logits)\n",
    "print(pred_prob.argmax()) #找到所有softmax之后最大的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0217,  0.0247,  0.0014,  ..., -0.0141, -0.0031, -0.0160],\n",
      "        [ 0.0101,  0.0317,  0.0267,  ...,  0.0045, -0.0024,  0.0339]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0228, -0.0219], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0407,  0.0209,  0.0361,  ..., -0.0191,  0.0408, -0.0318],\n",
      "        [-0.0344, -0.0054, -0.0049,  ...,  0.0129, -0.0122, -0.0123]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0353, -0.0062], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0119,  0.0022, -0.0097,  ...,  0.0441,  0.0220,  0.0198],\n",
      "        [ 0.0334, -0.0044, -0.0152,  ...,  0.0124, -0.0192,  0.0386]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0145, -0.0027], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`感知机`由两个全连接层组成，采用`sigmoid`函数作为激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perception(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_feature, output_feature):\n",
    "        super(perception, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_feature, hidden_feature)\n",
    "        self.layer2 = nn.Linear(hidden_feature, output_feature)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight torch.Size([4, 3])\n",
      "layer1.bias torch.Size([4])\n",
      "layer2.weight torch.Size([1, 4])\n",
      "layer2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 初始化网络结构\n",
    "perceptron = perception(3,4,1)\n",
    "\n",
    "# 打印网络的参数结构\n",
    "for name, param in perceptron.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在functional中都有一个与之相对应的函数。`nn.functional`中的函数和`nn.Module`的主要区别在于，用`nn.Module`实现的layers是一个特殊的类，都是由`class layer(nn.Module)`定义，会自动提取可学习的参数；而`nn.functional`中的函数更像是纯函数，由`def function(input)`定义。下面将举例说明`functional`的使用，并对比二者的不同之处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class function(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(function, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型有可学习的参数，最好用 `nn.Module`，否则既可以使用`nn.functional`也可以使用`nn.Module`\n",
    "\n",
    "- 激活函数`（ReLU、sigmoid、tanh）`、池化`（MaxPool）`等层没有可学习参数，可以使用对应的functional函数代替\n",
    "- 卷积、全连接等具有可学习参数的网络建议使用`nn.Module`。\n",
    "- 虽然`dropout`操作也没有可学习操作，但建议还是使用`nn.Dropout`而不是`nn.functional.dropout`，因为dropout在训练和测试两个阶段的行为有所差别，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个resblock的结构图：(input-> conv -> bn -> ReLU ->conv ->BN )+ input -> ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,input_channel, output_channel, stride = 1, shortcut = None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, 3, stride, 1, bias = False),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(output_channel, output_channel, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(output_channel)\n",
    "        )\n",
    "        self.right = shortcut\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        res = x if self.right is None else self.right(x)\n",
    "        out += res\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相当于是res net中包含多个layer，而每一个layer中包含多个res block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes = 1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(3, 2, 1)\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, 3, 1, is_shortcut=False)\n",
    "        self.layer2 = self._make_layer(64, 128, 4, 2)\n",
    "        self.layer3 = self._make_layer(128, 256, 6, 2)\n",
    "        self.layer4 = self._make_layer(256, 512, 3, 2)\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self,inchannel, outchannel, block_num, stride, is_shortcut = True):\n",
    "        if is_shortcut:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, 1, stride, bias = False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "        else:\n",
    "            shortcut = None\n",
    "        layers = []\n",
    "        layers.append(ResBlock(inchannel, outchannel, stride, shortcut))\n",
    "\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResBlock(outchannel, outchannel)) #注意这个地方的维度要保持和输入一致\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet()\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "out = resnet(input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
