{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从接口的角度来讲，对Tensor的操作可分为两类：\n",
    "\n",
    "- torch.function，如torch.save等。\n",
    "- tensor.function，如tensor.view等。\n",
    "\n",
    "为方便使用，对Tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum(a, b)与a.sum(b)`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对Tensor的操作又可分为两类：\n",
    "\n",
    "- 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的Tensor。\n",
    "- 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以_结尾的都是inplace方式，即会修改调用者自己的数据，在实际应用中需加以区分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list转tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([])\n",
      "tensor([])\n",
      "[[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "a = t.tensor(data) #生成以一个和之前形状一样的tensor\n",
    "c = t.Tensor() # 直接创建空tensor\n",
    "d = t.tensor(())\n",
    "print(a)\n",
    "print(c)\n",
    "print(d)\n",
    "\n",
    "#list转list\n",
    "b = a.tolist()\n",
    "print(b) #将tensor转化回list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_like初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6879, 0.8007],\n",
      "        [0.7155, 0.1374]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_data = t.tensor([[1,2],[3,4]])\n",
    "x_ones = t.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = t.rand_like(x_data, dtype=t.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生特定形状的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[5, 5, 6],\n",
      "        [9, 3, 4]]) \n",
      "\n",
      "one tensor: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "zero tensor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "randint tensor: tensor([[5, 5, 6],\n",
      "        [9, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "one_data = t.ones(shape)\n",
    "rand_data = t.rand(shape)\n",
    "zero_data = t.zeros(shape)\n",
    "randint_data = t.randint(3, 10, shape)\n",
    "print(f\"Random Tensor: \\n {randint_data} \\n\")\n",
    "print(f\"one tensor: {one_data}\")\n",
    "print(f\"zero tensor: {zero_data}\")\n",
    "print(f\"randint tensor: {randint_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor3 = t.tensor((), dtype=t.int32) #通过指定对应的类型\n",
    "tensor3.new_ones((2, 3)) #对空tensor进行重新设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生随机排列的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 2, 6, 3, 7, 5, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个长度为8的随机排列的Tensor\n",
    "t.randperm(8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生对角线为1，其余为0的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个对角线为1，其余为0的Tensor,不要求行列数一致\n",
    "t.eye(2, 3, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个起始值为1，上限为6，步长为2的Tensor\n",
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个均匀间距的Tensor，将1到10的数分为3份\n",
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor元素个数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "x_data = t.randn(2,3)\n",
    "print(x_data.numel())\n",
    "print(x_data.nelement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x_data.size())\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "tensor 's type:torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "tensor = t.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "print(f\"tensor 's type:{tensor.type()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表3-2: Tensor数据类型\n",
    "\n",
    "| Data type       | dtype                        | CPU tensor         | GPU tensor            |\n",
    "|-----------------|------------------------------|--------------------|----------------------|\n",
    "| 32-bit浮点型    | `torch.float32` or `torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor` |\n",
    "| 64-bit浮点型    | `torch.float64` or `torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit半精度浮点型 | `torch.float16` or `torch.half` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |\n",
    "| 8-bit无符号整型 | `torch.uint8`                | `torch.ByteTensor` | `torch.cuda.ByteTensor` |\n",
    "| 8-bit有符号整型 | `torch.int8`                 | `torch.CharTensor` | `torch.cuda.CharTensor` |\n",
    "| 16-bit有符号整型 | `torch.int16` or `torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor` |\n",
    "| 32-bit有符号整型 | `torch.int32` or `torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor` |\n",
    "| 64-bit有符号整型 | `torch.int64` or `torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor` |\n",
    "\n",
    "不同Tensor类型之间的转换方法\n",
    "\n",
    "1. **数据类型转换**：各种数据类型之间可以互相转换，`type(new_type)` 是通用的做法，同时还有 `float`、`long`、`half` 等快捷方法。\n",
    "\n",
    "2. **CPU与GPU Tensor转换**：CPU Tensor与GPU Tensor之间的互相转换通过 `tensor.cuda` 和 `tensor.cpu` 方法实现，此外还可以使用 `tensor.to(device)`。\n",
    "\n",
    "3. **创建同种类型的张量**：\n",
    "   - 使用 `torch.*_like` 和 `tensor.new_*`。例如，`torch.ones_like()` 可以生成和 `tensorA` 拥有同样属性（类型，形状，CPU/GPU）的新Tensor。\n",
    "   - `tensor.new_*(new_shape)` 可以新建一个不同形状的Tensor，例如 `tensor.new_zeros()`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断cuda是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    x_data = x_data.to(\"cuda\")\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切片和索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = t.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0957,  0.1117, -0.1136, -1.3344],\n",
      "        [ 0.6497, -1.6803,  0.0482, -0.4707],\n",
      "        [-1.1186, -0.1281, -1.1201,  0.4486]])\n"
     ]
    }
   ],
   "source": [
    "a = t.randn(3,4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看第1行结果： tensor([-0.0957,  0.1117, -0.1136, -1.3344])\n",
      "查看第2列结果： tensor([ 0.1117, -1.6803, -0.1281])\n",
      "查看第2行最后两个元素： tensor([ 0.0482, -0.4707])\n"
     ]
    }
   ],
   "source": [
    "print(\"查看第1行结果：\", a[0])\n",
    "print(\"查看第2列结果：\", a[:,1])\n",
    "print(\"查看第2行最后两个元素：\", a[1, -2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False, False],\n",
      "        [ True, False,  True, False],\n",
      "        [False, False, False,  True]])\n",
      "tensor([[0, 1, 0, 0],\n",
      "        [1, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 返回一个ByteTensor\n",
    "print(a > 0) # bool型\n",
    "print((a > 0).int()) # 整型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1117, 0.6497, 0.0482, 0.4486])\n",
      "tensor([0.1117, 0.6497, 0.0482, 0.4486])\n",
      "tensor([[-0.0957,  0.1117, -0.1136, -1.3344],\n",
      "        [ 0.6497, -1.6803,  0.0482, -0.4707],\n",
      "        [-1.1186, -0.1281, -1.1201,  0.4486]])\n",
      "tensor([[0.0000, 0.1117, 0.0000, 0.0000],\n",
      "        [0.6497, 0.0000, 0.0482, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4486]])\n"
     ]
    }
   ],
   "source": [
    "# 返回Tensor中满足条件的结果，下面两种写法等价\n",
    "# 选择返回的结果与原Tensor不共享内存空间\n",
    "print(a[a > 0])\n",
    "print(a.masked_select(a>0)) \n",
    "print(a)\n",
    "\n",
    "# 用torch.where保留原始的索引位置，不满足条件的位置置0\n",
    "print(t.where(a > 0, a, t.zeros_like(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view的操作，将原来的tensor的形状转化为4*4的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = t.arange(0, 16).view(4, 4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取对角线上的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = t.tensor([[0,1,2,3]])\n",
    "a.gather(0, index) #dim = 0表示行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选取反对角线上的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.tensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index) #dim = 1 表示对列进行提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选取两条对角线上的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.tensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7]],\n",
       "\n",
       "        [[ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,16).view(2,2,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([14,  4])\n",
      "tensor([9, 1])\n"
     ]
    }
   ],
   "source": [
    "# 从一个三维的tensor中获取对应位置的元素可以表示为：\n",
    "print(x[0,1,2]) #表示第0行第1列第2个元素\n",
    "\n",
    "print(x[[1,0],[1,1],[2,0]]) #表示第1行第1列第2个元素和第0行第1列第0个元素\n",
    "\n",
    "print(x[[1, 0], [0], [1]]) #表示x[1,0,1]和x[0,0,1], 如果对应位置只有一个数字表示都采用这个数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐元素操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).float().view(2, 3)\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [0., 1., 2.]])\n",
      "tensor([[0., 1., 2.],\n",
      "        [0., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# 表示取模操作，下面两种写法等价\n",
    "print(a % 3)\n",
    "print(t.fmod(a, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[1., 1., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 对tensor元素进行上下限截断保持在给定的范围中\n",
    "print(a)\n",
    "print(t.clamp(a, min=1, max=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归并操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归并操作会使用Tensor中的部分元素进行计算，其输出形状可能小于输入形状。同时我们可以沿着某一维度进行指定的归并操作。如加法`sum`，既可以计算整个Tensor的和，也可以计算Tensor中每一行或每一列的和。常用的归并操作如表3-5所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "其中size中是否有`\"1\"`，取决于参数`keepdim`，若指定`keepdim=True`则会保留维度1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.]]) torch.Size([1, 3])\n",
      "tensor([2., 2., 2.]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 注意对比是否保留维度1的区别\n",
    "print(b.sum(dim=0, keepdim=True ),b.sum(dim=0, keepdim=True ).shape) #dim=0表示对列进行求和，相当于是保留下了列\n",
    "print(b.sum(dim=0, keepdim=False), b.sum(dim=0, keepdim=False).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cumsum这个操作相当于是按照行或者列 累加一直堆下去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n",
      "tensor([[ 2,  5,  9],\n",
      "        [ 5, 11, 18]])\n"
     ]
    }
   ],
   "source": [
    "a = t.arange(2, 8).view(2, 3)\n",
    "print(a)\n",
    "print(a.cumsum(dim=1)) # 沿着行累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [ True,  True,  True]])\n",
      "a中大于b的元素:  tensor([ 9., 12., 15.])\n",
      "a中最大的元素:  tensor(15.)\n"
     ]
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "print(a > b)\n",
    "print(\"a中大于b的元素: \", a[a > b]) # 返回a中大于b的元素\n",
    "print(\"a中最大的元素: \", t.max(a)) # 返回a中最大的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15., 12.,  9.],\n",
      "        [ 6.,  3.,  0.]])\n",
      "torch.return_types.max(\n",
      "values=tensor([15., 12.,  9.]),\n",
      "indices=tensor([0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(t.max(b, dim = 0)) # 返回b中每一列最大的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a, b) # 两个Tensor对应位置上较大的元素\n",
    "# 注意这个要求必须两个tensor的大小是一样的才有可比性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼接操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1189, 0.4205, 0.8536, 0.1189, 0.4205, 0.8536, 0.1189, 0.4205, 0.8536],\n",
      "        [0.3248, 0.4944, 0.5703, 0.3248, 0.4944, 0.5703, 0.3248, 0.4944, 0.5703]])\n",
      "tensor([[0.1189, 0.4205, 0.8536],\n",
      "        [0.3248, 0.4944, 0.5703],\n",
      "        [0.1189, 0.4205, 0.8536],\n",
      "        [0.3248, 0.4944, 0.5703],\n",
      "        [0.1189, 0.4205, 0.8536],\n",
      "        [0.3248, 0.4944, 0.5703]])\n"
     ]
    }
   ],
   "source": [
    "x_ten= t.rand((2,3))\n",
    "t1 = t.cat([x_ten, x_ten, x_ten], dim=1) #dim = 1表示增加列\n",
    "print(t1) \n",
    "t2 = t.cat([x_ten, x_ten, x_ten], dim=0) #dim = 0表示增加行\n",
    "print(t2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算术运算\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1851, 0.2463],\n",
      "        [0.2463, 0.3928]])\n",
      "tensor([[0.1851, 0.2463],\n",
      "        [0.2463, 0.3928]])\n",
      "tensor([[0.0808, 0.0796, 0.0247],\n",
      "        [0.1023, 0.0912, 0.1993]])\n",
      "tensor([[0.0808, 0.0796, 0.0247],\n",
      "        [0.1023, 0.0912, 0.1993]])\n"
     ]
    }
   ],
   "source": [
    "tensor = t.rand(2, 3)\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T) #表示矩阵乘法\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor) #表示矩阵中按照element-wise进行乘法运算\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算元素查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6891863346099854 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 元素替换 in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9854, 0.7586, 0.3820],\n",
      "        [0.1077, 0.9946, 0.5401]]) \n",
      "\n",
      "tensor([[5.9854, 5.7586, 5.3820],\n",
      "        [5.1077, 5.9946, 5.5401]])\n",
      "tensor([[5.9854, 5.7586, 5.3820],\n",
      "        [5.1077, 5.9946, 5.5401]])\n"
     ]
    }
   ],
   "source": [
    "tensor = t.rand((2,3))\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "tensor_y = t.rand((2,3))\n",
    "tensor_y.copy_(tensor) #这个操作必须是初始化之后才能使用\n",
    "print(tensor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0000, 5.1970, 5.5264],\n",
      "        [4.0000, 5.7325, 5.1879]])\n",
      "tensor([[4.0000, 5.1970, 5.5264],\n",
      "        [4.0000, 5.7325, 5.1879]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_y)\n",
    "tensor_y[:,0].fill_(4) #将第一列的元素全部替换成4，inplace操作\n",
    "print(tensor_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor与Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "th = t.ones(5)\n",
    "print(f\"t: {th}\")\n",
    "n = th.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "th.add_(1)\n",
    "print(f\"t: {th}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy转Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = t.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改变tensor的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看tensor对应的性质属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape=torch.Size([2, 3, 4]). a.size()=torch.Size([2, 3, 4])\n",
      "这是个3维Tensor, 总共24个元素\n"
     ]
    }
   ],
   "source": [
    "tensor = t.arange(24).reshape(2, 3, 4)\n",
    "# tensor.shape 和tensor.size() 等价\n",
    "print(f\"a.shape={tensor.shape}. a.size()={tensor.size()}\")\n",
    "print(f'这是个{tensor.dim()}维Tensor, 总共{tensor.numel()}个元素')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵的转置会导致存储空间不连续，因此需调用它的.contiguous方法将其转为连续"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  3.,  6.],\n",
      "        [ 9., 12., 15.]])\n",
      "tensor([[ 0.,  9.],\n",
      "        [ 3., 12.],\n",
      "        [ 6., 15.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "print(a)\n",
    "b = a.t()\n",
    "print(b)\n",
    "b.is_contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.contiguous()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有改变Tensor形状的操作都可以通过tensor.reshape实现。`tensor.reshape(new_shape)`会把不连续的`Tensor`变成连续的再进行形状变化，这一操作等价于`tensor.contiguous().view(new_shape)`。关于view和reshape的选用可参考下面的建议:\n",
    "\n",
    "- 对于reshape而言:其接口更便捷，`会自动把不连续的Tensor变为连续的`，能避免很多报错。同时它的函数名和NumPy一样，便于使用。\n",
    "- 对于view而言:其函数名更短，而且Tensor经过view操作之后仍然共享存储空间。当我们并不希望改变形状之后的Tensor和原Tensor共享存储时可以使用view方法。\n",
    "  \n",
    "reshape和view二者之间有一些区别：如果对Tensor使用了transpose、permute等操作，会造成Tensor的内存变得不连续，而view方法只能改变连续的张量，所以需要先调用`contiguous()`方法，但reshape方法则不受此限制。具体来说，view方法返回的Tensor和原Tensor共享Storage（注意：不是共享内存地址），而reshape方法可能返回原Tensor的copy或者共享Storage的view。如果当前满足连续性条件，则结果与view相同；否则返回的就是copy（此时等价于`tensor.contiguous().view()`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12]) torch.Size([2, 6]) torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "a = t.arange(1, 13)\n",
    "b = a.view(2, 6)\n",
    "c = a.reshape(2, 6) # 此时view和reshape等价，因为Tensor是contiguous\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "b = b.t() # b 不再连续\n",
    "print(b.reshape(-1, 4)) # reshape 可以\n",
    "\n",
    "# 下面会报错，view无法在改变数据存储的情况下进行形状变化\n",
    "# b.view(-1, 4)\n",
    "print(b.contiguous().view(-1,4))\n",
    "# 注意这里的改变形状都不是本身改变的，而是产生了一个新的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape操作的难点在于如何快速灵活地指定形状，常用的快捷变形方法有：\n",
    "\n",
    "- `tensor.view(dim1,-1,dimN)`: 在调整Tensor形状的时候，我们不需要指定每一维的形状，可以把其中一个维度指定为-1，PyTorch会自动计算对应的形状\n",
    "- `tensor.view_as(other)`: 将Tensor的形状变为和other 一样，等价于tensor.view(other.shape)\n",
    "- `tensor.squeeze()`: 将Tensor形状中为1的维度去掉，比如(1,3,1,4)变为(3,4)\n",
    "- `tensor.flatten(start_dim=0, end_dim=-1)`: 将Tensor形状中某些连续维度合并为一个维度. 比如形状(2,3,4,5) ->(2,12,5)。\n",
    "- `tensor[None]` 和 `tensor.unsqueeze(dim)`: 为Tensor新建一个维度，并把形状设为1, 比如形状(2,3) -> (2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n",
      "torch.Size([3, 20]) torch.Size([3, 5, 4])\n",
      "torch.Size([3, 5, 1, 4])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "H, W = 4, 5\n",
    "img_3xHxW = t.randn(3, H, W)\n",
    "\n",
    "# 目标数据排列和输入一样，直接使用reshape\n",
    "img_3xHW = img_3xHxW.reshape(3, -1)\n",
    "print(img_3xHW.shape)\n",
    "\n",
    "# 目标数据排列和输入不一样，先通过transpose变成(3,W,H), 再变成(3,WH)\n",
    "img_3xWH = img_3xHxW.transpose(1, 2)\n",
    "print(img_3xHW.shape, img_3xWH.shape)\n",
    "\n",
    "img_a = img_3xWH.unsqueeze(dim = 2)\n",
    "print(img_a.shape)\n",
    "\n",
    "img_b = img_a.squeeze(dim = 2).flatten(start_dim = 1, end_dim = -1)\n",
    "print(img_b.shape)\n",
    "\n",
    "# 再变形为3xWxH的形式\n",
    "img_3xWxH = img_3xWH.reshape(3, W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
